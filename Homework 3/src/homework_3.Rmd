---
title: "Homework 3"
subtitle: "Stat 597a: Spatial Models"
author: "Claire Kelling"
date: "Due October 31, 2017"
output: pdf_document
---
```{r setup, include=FALSE, warning=FALSE}
knitr::opts_chunk$set(fig.align="center")
knitr::opts_chunk$set(fig.width=8, fig.height=5)
library(geoRglm)
library(sp)
library(gstat)
library(fields)
library(classInt)
library(ggplot2)
library(maps)
library(mcmcse)
library(maptools)
library(spdep)
library(tmap)
library(dplyr)
library(MCMCpack)
library(mvtnorm)
set.seed(124)

```

## Problem 1: 
\textbf{Fit a linear model relating rent per square meter to the covariates using least squares, and extract the coefficient estimates. You can ignore the Location variable for now since we will later treat this as a random effect. Also note that the room indicator variables include one that is redundant, so treat a single room as the baseline (i.e., leave Room1 out of the model, so the intercept corresponds to a single room and coefficients for the others represent adjustments to the intercept for a different number of rooms).}

For this problem, I fit a linear model, but I do not include the intercept, because there is no intercept included in part 4 for the spatial model. As instructed, I also do not include Location or Room1 in this model. I include the estimates for the coefficients below.
```{r 1, echo=FALSE, warning=FALSE,message=FALSE, include = TRUE}

#load the data
load("C:/Users/ckell/OneDrive/Penn State/2017-2018/597/spatial_statistics_597/Homework 3/data/munichrents.RData")

#head(rents)

#Fit a linear model relating rent per square meter to the covariates using least squares
#  This model is fit without Location (used this later as a random effect)
#  This model is also fit without Room1 because we treat a single room as the baseline
lin_mod <- lm(RentPerM2~ Year+ NoHotWater+NoCentralHeat+ NoBathTiles+
                SpecialBathroom + SpecialKitchen +Room2+Room3 +Room4 +Room5+ Room6, data=rents)
#summary(lin_mod)

#extract the coefficient estimates
beta <- lin_mod$coefficients
beta

```


## Problem 2: 
\textbf{There are two SpatialPolygons objects associated with this dataset, districts.sp and parks.sp. The first corresponds to city districts in which apartments may be located. The second corresponds to districts with no possible apartments, such as parks or fields.
Create an nb object with neighbors for the districts, defining neighbors as districts that share a common boundary. Make a plot showing the districts, then add the parks shaded a different color. Think about the way parks are treated; what else could we do?}

First, I created an nb object with neighbors for the districts. I use the rook pattern because the problem states to define neighbors as districts that share a common boundary. The queen pattern would define neighbors as those districts which share a boundary and/or a corner. 

In my first plot below, I include a plot showing the districts, with the parks added in a different color (green). In my second plot below, I also add the neighborhood structure to this first map. In the context of this assignment, we do not treat the parks as districts. In other words, these parks are ignored and treated as empty. 

Alternatively, perhaps we should consider districts that both share a border with the park to also be neighbors, as they are probably close to eachother. 

```{r 2, echo=FALSE, warning=FALSE,message=FALSE, include = FALSE}

plot(parks.sp)
plot(districts.sp)

#Create an nb object with neighbors for the districts, defining neighbors as districts that 
#share a common boundary.
#queen is the option that has neigbor being at least one point shared, including corners
#rook does not include corners, only borders
district_neighb <- poly2nb(districts.sp, queen = TRUE)

```

```{r 2b, echo=FALSE, warning=FALSE,message=FALSE, include = TRUE}
par(mfrow = c(1,2))

#plot the neighborhoods
coords <- coordinates(districts.sp)
plot(districts.sp, main = "Districts with Neighbors")
plot(district_neighb, coords, col="blue", pch = 19, cex = 0.6, add = TRUE)


#Make a plot showing the districts, then add the parks shaded a different color.
plot(districts.sp, main = "Map of the Districts, Parks in Green")
plot(district_neighb, coords, col="blue", pch = 19, cex = 0.6, add = TRUE)
plot(parks.sp,col = "lightgreen", add=TRUE)


```


## Problem 3:
\textbf{There are 380 districts in districts.sp, and the corresponding district numbers are indicated by the Location variable in rents. How many of the 380 districts appear in the rent dataset?
I've included a matrix H that provides a mapping between the districts as they're ordered in districts.sp and as they appear in the rents dataframe. Use H to create a new vector containing the number of observations in each district, and make a color or grayscale plot to illustrate this. Note that inference for unobserved districts will still be possible under a hierarchical mixed effects model, since we can "borrow strength" from nearby districts that do have observations.}

312 of the 380 districs appear in the rent dataset. Below, I include a map with the number of apartments in the rent dataset for each district. Once again, I include the empty districts with a red etching.

```{r 3, echo=FALSE, warning=FALSE,message=FALSE, include = FALSE}
#How many of the 380 districts appear in the rent dataset?
length(unique(rents$Location))
# There are 312 districts represented in the rents dataset, of the 380 total districts.
 
# Use H to create a new vector containing the number of observations in each district
# There are 380 districts, so I want a 1x380 or 380x1 vector
dim(H) #H is of dimension 2035X380
# So, I need (1x2035)(2035x380) = 1x380
rents$indicator <- rep(1, nrow(rents))
num_per_dist <- rents$indicator%*% H
dim(num_per_dist) # this is my 1x380 vector
sum(num_per_dist) # This is equal to 2035, the total number of observations (rows) in the rent dataset
num_per_dist <- as.numeric(num_per_dist)

#Now I will create the data structure that I need to create a plot
sp_f <- fortify(districts.sp)
#head(sp_f)
districts.sp$id <- row.names(districts.sp)
#head(districts.sp@data, n=2)
districts.sp@data$num <- num_per_dist
sp_f <- left_join(sp_f, districts.sp@data)
parks.sp <- fortify(parks.sp)
num <- num_per_dist

#make a color or grayscale plot to illustrate this
obs_by_dist <- ggplot(data = sp_f, aes(long, lat, group = group, fill = num)) + geom_polygon(linetype=0) + coord_equal() + 
  labs(fill = "No. of \nObservations") +
  ggtitle("Number of Observations per District")+ scale_fill_gradient(low = "lightblue", high = "darkblue")

obs_by_dist <- ggplot() + geom_polygon(data = sp_f, aes(long, lat, group = group, fill = num)) + coord_equal() +
  labs(fill = "No. of \nObservations") +
  ggtitle("Number of Observations per District")+ scale_fill_gradient(low = "lightblue", high = "navyblue")

#final graph
obs_graph <- obs_by_dist + geom_polygon(data=sp_f,aes(long,lat, group = group), fill = NA, col = "black")+geom_polygon(data = parks.sp, aes(parks.sp$long,parks.sp$lat,group= parks.sp$group), fill = "darkseagreen1")+geom_polygon(data=parks.sp, aes(long, lat, group = group), fill = NA, col = "black")


```

```{r, echo=FALSE, warning=FALSE,message=FALSE}
par(mfrow= c(1,1))
print(obs_graph)
```
## Problem 4
\textbf{We will now create a Gibbs sampler to sample from the posterior distribution under the following Bayesian model. Let X be the matrix of covariates, including the intercept term. Let n be the number of data points in Y and m be the number of spatial locations in $\eta$.}
Data model:
$$ Y|\beta,\eta,\sigma^2 \sim N(X \beta + H\eta,\sigma^2I) $$
Process model:
$$ p(\eta|\tau^2) \propto (\tau^2)^{-(m-1)/2}\exp\{\frac{-1}{2\tau^2}\eta^T(D_w-W)\eta\} $$
where W is the matrix of 0's and 1's indicating the neighborhood structure from problem 2, and $D_w$ is a diagonal matrix with diagonal entries $\sum_j W_{1j},...,\sum_j W_{nj}$. That is, $\eta$ follows an (improper) intrinsic autoregressive model. The $-(m-1)/2$ exponent on $\tau^2$ is due to the fact that the matrix $D_w -???? W$ has rank m-1 rather than m.

Prior model: Specify independent priors for $\beta$, $\sigma^2$, and $\tau^2$, with
$$ p(\beta) \propto 1, \text{   and   } \sigma^2, \tau^2 \sim InverseGamma(0:001, 0:001)$$
The full conditional distributions for $\beta$, $\eta$ $\sigma^2$, and $\tau^2$ are given at the end of this assignment. Construct a Gibbs sampler that cycles through each of the full conditionals and stores the results for B = 10,000 iterations. The full conditionals are given below.

A few notes to keep in mind when constructing the sampler:
\begin{itemize}
\item The matrix W can be computed from your nb object in problem 2; see help(nb2mat). I also included objects X and y with the data file.
\item The function rinvgamma is in the library MCMCpack.
\item IMPORTANT: The intrinsic autoregressive model is an example of a pair-wise difference prior. It defines proper distributions for the differences $n_i-n_j$, but it also implicitly contains a distribution for $\frac{1}{m}\sum_{i=1}^{m}\eta_i$ that has infinite variance. In practice, since there is also an intercept term in $X\beta$, we impose the constraint $\sum_{i=1}^{m}\eta_i =0$ when we sample from the full conditional for $\eta$. \textbf{Do this numerically by subtracting the mean $\frac{1}{m}\sum_{i=1}^{m}\eta_i^{(j)}$ from $\eta^{(j)}$ in each iteration j.}

\end{itemize}

```{r 4, echo=FALSE, warning=FALSE,message=FALSE, include = FALSE}
# Construct a Gibbs sampler that cycles through each of the full conditionals and stores 
# the results for B = 10,000 iterations.

# I will start by creating the input parameters/vectors/matrices
n <-  nrow(rents) # number of data points in Y
m <- nrow(districts.sp) # number of spatial locations in eta
W <- nb2mat(neighbours = district_neighb, style = "B")
#W is matrix of 0's and 1's indicating neighb structure from 2, need style = B to be binary (W is row standardized)
#X and y are given in the data file
D <- diag(rowSums(W)) # diagonal matrix with row sums of W

#is there a typo in this question? row sum up to m, not n?

#how many iterations
B <- 10000

#Prior parameters
b <- 1
a.s2 <- 0.001; b.s2 <- 0.001
a.t2 <- 0.001; b.t2 <- 0.001

```

```{r 4a, echo=FALSE, warning=FALSE,message=FALSE,eval=FALSE,include = FALSE}
#setup and storage and starting values
beta.samps <- matrix(NA, nrow = 12, ncol = B)
beta.samps[,1] <- beta 
#coefficients from problem 1

s2.samps <- t2.samps <- rep(NA, B)
#eta.obs.samps <- matrix(NA, nrow = n, ncol = B)
s2.samps[1] <- t2.samps[1] <- 1

eta.obs.samps <- matrix(NA, nrow = m, ncol = B)
eta.obs.samps[,1] <- rep(mean(y), m)
y <- as.matrix(y)

## MCMC sampler
for(i in 2:B){
  print(i)
  
  ## beta_obs | Rest
  mu_b <- solve(t(X)%*%X)%*%t(X)%*%(y-H%*%as.matrix(eta.obs.samps[,i-1]))
  Sig_b <- s2.samps[i-1]*solve(t(X)%*%X)
  beta.samps[,i] <- rmvnorm(1, mean= mu_b, sigma = Sig_b, method = "svd") 
   
  ## eta | Rest
  mu_e <- solve((t(H)%*%H)/s2.samps[i-1] + (D-W)/t2.samps[i-1])%*%t(H)%*%(y-X%*%beta.samps[,i])/s2.samps[i-1]
  Sig_e <- solve(t(H)%*%H/s2.samps[i-1] + (D-W)/t2.samps[i-1])
  eta.pre <- rmvnorm(1, mean = mu_e, sigma = Sig_e, method = "svd")
  #need to adjust for the pairwise difference prior
  eta.obs.samps[,i] <- eta.pre - mean(eta.pre) 
  
  
  ## s2 | Rest
  a <- 0.001 + (n/2)
  resid <- y-X%*%beta.samps[,i]
  second_part <- H%*%as.matrix(eta.obs.samps[,i])
  b <- 0.001 + 0.5*(t(resid-second_part)%*%(resid-second_part))
  s2.samps[i] <- rinvgamma(1, a, b)

  ## t2 | Rest
  c <- 0.001 + ((m-1)/2)
  d <- 0.001 + 0.5*(t(as.matrix(eta.obs.samps[,i]))%*%(D-W)%*%as.matrix(eta.obs.samps[,i]))
  t2.samps[i] <- rinvgamma(1, c, d)

}

save(t2.samps, s2.samps, beta.samps, eta.obs.samps, file =  "C:/Users/ckell/OneDrive/Penn State/2017-2018/597/spatial_statistics_597/Homework 3/data/mcmc_data.RData")

```

```{r 4b, echo=FALSE, warning=FALSE,message=FALSE, include = FALSE}
load("C:/Users/ckell/OneDrive/Penn State/2017-2018/597/spatial_statistics_597/Homework 3/data/mcmc_data2.RData")
## Diagnostics

plot(beta.samps[1,], type = "l")
plot(s2.samps, type = "l")
plot(t2.samps, type = "l")
plot(eta.obs.samps[1,], type = "l")

burnin <- 300
s2.final <- s2.samps[-(1:burnin)]
t2.final <- t2.samps[-(1:burnin)]
beta.final <- beta.samps[,-(1:burnin)]
eta.obs.final <- eta.obs.samps[,-(1:burnin)]

acf(s2.final)
acf(t2.final)
acf(beta.final[1,])
acf(eta.obs.final[1,])

effectiveSize(s2.final)
effectiveSize(t2.final)
effectiveSize(beta.final[1,])
effectiveSize(eta.obs.final[1,])

#table with posterior means and credible intervals
b_post_mean <- apply(beta.final, 1, mean)
b_CI <- apply(beta.final, 1, function(x) quantile(x, probs = c(0.025, 0.975)))
est_and_ci <- cbind(b_CI[1,], b_post_mean, b_CI[2,])

est_and_ci <- cbind(est_and_ci, beta)

colnames(est_and_ci) <- c("0.025 Quantile", "Posterior Mean", "0.975 Quantile", "Initial Estimate")
rownames(est_and_ci) <- c("b1","b2","b3","b4","b5","b6","b7","b8","b9","b10","b11","b12")

## Find pointwise posterior means and sds
eta.m <- apply(eta.obs.final, 1, mean)
eta.sd <- apply(eta.obs.final, 1, sd)

districts.sp@data$eta.m <- eta.m
districts.sp@data$eta.sd <- eta.sd
#districts.sp@data <- districts.sp@data[,-c(2)]
sp_f <- left_join(sp_f, districts.sp@data)


m_by_dist <- ggplot() + geom_polygon(data = sp_f, aes(long, lat, group = group, fill = eta.m)) + coord_equal() +
  labs(fill = "No. of \nObservations") +
  ggtitle("Posterior mean of eta")+ scale_fill_gradient(low = "lightblue", high = "navyblue")

#final graph
m_map <- m_by_dist + geom_polygon(data=sp_f,aes(long,lat, group = group), fill = NA, col = "black")+geom_polygon(data = parks.sp, aes(parks.sp$long,parks.sp$lat,group= parks.sp$group), fill = "darkseagreen1")+geom_polygon(data=parks.sp, aes(long, lat, group = group), fill = NA, col = "black")

sd_by_dist <- ggplot() + geom_polygon(data = sp_f, aes(long, lat, group = group, fill = eta.sd)) + coord_equal() +
  labs(fill = "No. of \nObservations") +
  ggtitle("Standard Dev of Post eta")+ scale_fill_gradient(low = "lightblue", high = "navyblue")

#final graph
sd_map <- sd_by_dist + geom_polygon(data=sp_f,aes(long,lat, group = group), fill = NA, col = "black")+geom_polygon(data = parks.sp, aes(parks.sp$long,parks.sp$lat,group= parks.sp$group), fill = "darkseagreen1")+geom_polygon(data=parks.sp, aes(long, lat, group = group), fill = NA, col = "black")

```

\textbf{Full Conditionals:}
$$ \beta | Rest \sim N((X^TX)^{-1}X^T(Y-H_\eta), \sigma^2(X^TX)^{-1}) $$

$$ \eta|Rest \sim N([H^TH/\sigma^2 + (D_w-W)/\tau^2]^{-1}H^T(Y_X\beta)/\sigma^2, [H^TH/\sigma^2 +(D_w-W)/\tau^2]^{-1}) $$
$$ \sigma^2|Rest \sim InverseGamma(0.001 +n/2, 0.001 + (Y-X\beta-H\eta)^T(Y-X\beta-H\eta)/2) $$
$$ \tau^2|Rest \sim InverseGamma(0.001 + (m-1)/2, 0.001 + \eta^T(D_w-W)\eta/2) $$

I ran the Gibbs Sampler for the B=10,000 iterations and chose a burnin value of 300. After discarding the burnin, I include the trace and ACF plots for $\sigma^2$ and $\tau^2$ below. 

```{r, echo=FALSE, warning=FALSE,message=FALSE}
par(mfrow= c(2,1))

plot.ts(s2.final, type = "l", xlab = "iteration", ylab = "value",main = "Trace Plot, sigma^2")
plot(t2.final, type = "l", xlab = "iteration", ylab = "value",, main = "Trace Plot, tau^2")

acf(s2.final)
acf(t2.final)
```
Below, I also include a table with posterior means of the $\beta$'s and 95% credible intervals constructed using the 0.025 and 0.975 quantiles of the posterior samples. We also see that the initial estimates are somewhat close to the posterior mean.

```{r, echo=FALSE, warning=FALSE,message=FALSE}
print(est_and_ci)
```
Lastly, I include a color map of the posterior means for the vector $\eta$ as well as a map for the posterior standard deviations for the vector $\eta$. We see that the mean is highest in the center and that the standard deviation is highest near the border of the map, where there are the fewest neighbors.

```{r, echo=FALSE, warning=FALSE,message=FALSE}
print(m_map)
print(sd_map)
```


## Appendix: R Code
Below, I include all of the R code that has produced graphs and data.
```{r appendix, ref.label='1', eval = FALSE}
```
```{r appendix2, ref.label='2', eval = FALSE}
```
```{r appendix2b, ref.label='2b', eval = FALSE}
```
```{r appendix3, ref.label='3', eval = FALSE}
```
```{r appendix4, ref.label='4', eval = FALSE}
```
```{r appendix4a, ref.label='4a', eval = FALSE}
```
```{r appendix4b, ref.label='4b', eval = FALSE}
```